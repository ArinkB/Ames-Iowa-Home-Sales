{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Ames - Additional Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jump To:\n",
    "- [Feature Engineering](#fut_eng)\n",
    "- [EDA](#eda)\n",
    "    - [Correlation](#corr)\n",
    "    - [Distribution](#dist)\n",
    "- [Addition Engineering](#more)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reading in our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in dataset\n",
    "df = pd.read_csv('datasets/train_clean.csv')\n",
    "test_df = pd.read_csv('datasets/test_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more'></a>\n",
    "### Additional Engineering\n",
    "- Once I used One Hot Encoder (in Predictions notebook) I got some pretty good RMSEs 21016, however the test dataset and train dataset ended up with different number of columns (320 and 300)\n",
    "- Initially I removed the columns that were different in each dataset however my RMSE increased and my model became overfit\n",
    "- Then I added columns with the value 0 in each dataset to match the column numbers, gave me my worse Kaggle score at 39k, however my train dataset was the best yet at 20k RMSE \n",
    "- I will now look at the categorical columns and see if I can shrink them even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that iterates through the columns and returns a list of columns with selected name for both datasets\n",
    "def iter_col(column):\n",
    "    print([col for col in df.columns if column in col])\n",
    "    print([col for col in test_df.columns if column in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion that takes column names and drops the column from the dataset\n",
    "# I used kwargs instead of just column in the arguments, because the number of columns can range from 1 - ?\n",
    "def drop_cols(*kwargs):\n",
    "    df.drop([*kwargs], axis=1, inplace = True)\n",
    "    test_df.drop([*kwargs], axis=1, inplace =True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes rating and shrinks it down to 3-4 rating for both datasets\n",
    "def re_val(column):\n",
    "    df[column] = df[column].map({'Ex':'Gd', # Excellent becomes good\n",
    "                                 'Gd' : 'Gd', # Good remains\n",
    "                                 'TA' : 'Avg', # Changed TA to say average for consistency\n",
    "                                 'Fa': 'Po', # Fair is Poor\n",
    "                                 'Po': 'Po', # Poor is poor\n",
    "                                 'NotApp.' : 'NotApp.'}) # Can't magically have one appear\n",
    "    \n",
    "    # Do the same for test_df\n",
    "    test_df[column] = test_df[column].map({'Ex':'Gd',\n",
    "                                 'Gd' : 'Gd',\n",
    "                                 'TA' : 'Avg',\n",
    "                                 'Fa': 'Po',\n",
    "                                 'Po': 'Po',\n",
    "                                 'NotApp.' : 'NotApp.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop this time instead of .map because I would have to do 10 different changes\n",
    "\n",
    "def re_cat(column):# iterate through each row within that column and change value of row based on condition\n",
    "                            # bringing a 10 point rating down to 4\n",
    "    for row in df.index:\n",
    "        if df.loc[row, column] >= 8:\n",
    "            df.loc[row, column] = str('Ex')\n",
    "        elif df.loc[row, column] >= 6:\n",
    "            df.loc[row, column] = str('Gd')\n",
    "        elif df.loc[row, column] >= 3:\n",
    "            df.loc[row, column] = str('Avg')\n",
    "        else:\n",
    "            df.loc[row, column] = str('Po')    \n",
    "            \n",
    "    for row in test_df.index:\n",
    "        if test_df.loc[row, column] >= 8:\n",
    "            test_df.loc[row, column] = str('Ex')\n",
    "        elif test_df.loc[row, column] >= 6:\n",
    "            test_df.loc[row, column] = str('Gd')\n",
    "        elif test_df.loc[row, column] >= 3:\n",
    "            test_df.loc[row, column] = str('Avg')\n",
    "        else:\n",
    "            test_df.loc[row, column] = str('Po')         \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Garage Type', 'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual', 'Garage Cond']\n",
      "['Garage Type', 'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual', 'Garage Cond']\n"
     ]
    }
   ],
   "source": [
    "iter_col('Garage') # gves me a list of all the Garage Columns\n",
    "# Dropping majority of the Garage Columns, Will keep Garage Area & Garage Cond\n",
    "drop_cols('Garage Type', 'Garage Finish','Garage Area', 'Garage Qual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Garage Cars', 'Garage Cond']\n",
      "['Garage Cars', 'Garage Cond']\n"
     ]
    }
   ],
   "source": [
    "iter_col('Garage') # What's left in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_val('Garage Cond') # Re evaluate the values into smaller bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Avg        1868\n",
       "NotApp.     114\n",
       "Po           55\n",
       "Gd           14\n",
       "Name: Garage Cond, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Garage Cond'].value_counts() #What's left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating dummy vars for both datasets\n",
    "# df = pd.concat([df, pd.get_dummies(df[['Garage Cond']], drop_first=True)], axis=1) \n",
    "# test_df = pd.concat([df, pd.get_dummies(df[['Garage Cond']], drop_first=True)], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Columns\n",
    "I'll keep both columns, but I want to change the rating from 1 - 10 to a smaller rating so that when I dummify the \n",
    "columns later there are only 4 new columns instead of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Overall Qual', 'Overall Cond']\n",
      "['Overall Qual', 'Overall Cond']\n"
     ]
    }
   ],
   "source": [
    "iter_col('Overall') # One Rates the home based on the overall condition, the other based on overall quality of material      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_cat('Overall Qual')\n",
    "re_cat('Overall Cond')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gd     397\n",
       "Avg    340\n",
       "Ex     137\n",
       "Po       4\n",
       "Name: Overall Qual, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Overall Qual'].value_counts() # A view of what it looks like now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dummies of Overall Quality and Condition columns\n",
    "# df = pd.concat([df, pd.get_dummies(df[['Overall Qual']], drop_first=True)], axis=1) \n",
    "# test_df = pd.concat([df, pd.get_dummies(df[['Overall Qual']], drop_first=True)], axis=1) \n",
    "\n",
    "# df = pd.concat([df, pd.get_dummies(df[['Overall Cond']], drop_first=True)], axis=1) \n",
    "# test_df = pd.concat([df, pd.get_dummies(df[['Overall Cond']], drop_first=True)], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original non-dummied columns\n",
    "drop_cols('Overall Qual', 'Overall Cond')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pool\n",
    "- In our cleanup column we dummied this to show whether the house has a pool or not, I am going to drop the Pool Area column as it was pretty closely correlated with 'Has Pool' and I'll dummy the Pool Quality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pool Area', 'Pool QC', 'Has Pool']\n",
      "['Pool Area', 'Pool QC', 'Has Pool']\n"
     ]
    }
   ],
   "source": [
    "iter_col('Pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_val('Pool QC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NotApp.    2042\n",
       "Gd            5\n",
       "Avg           2\n",
       "Po            2\n",
       "Name: Pool QC, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Pool QC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df, pd.get_dummies(df[['Pool QC']], drop_first=True)], axis=1) \n",
    "# test_df = pd.concat([df, pd.get_dummies(df[['Pool QC']], drop_first=True)], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols('Pool Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heating\n",
    "- Will go with the condition of the Heating instead of the type, most times when purchasing a home, the question is how old the furnace is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Heating', 'Heating QC']\n",
      "['Heating', 'Heating QC']\n"
     ]
    }
   ],
   "source": [
    "iter_col('Heating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gd     1384\n",
       "Avg     597\n",
       "Po       70\n",
       "Name: Heating QC, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_val('Heating QC')\n",
    "df['Heating QC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols('Heating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Electrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBrkr    814\n",
       "FuseA     48\n",
       "FuseF     15\n",
       "FuseP      1\n",
       "Name: Electrical, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Electrical'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./datasets/train_clean.csv', index = False)\n",
    "# test_df.to_csv('./datasets/test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make sure nothing came back\n",
    "print('null values left on Train', df.isnull().sum().sum())\n",
    "print('null values left on Test', test_df.isnull().sum().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
